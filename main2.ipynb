{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T23:12:53.634025Z",
     "iopub.status.busy": "2024-04-17T23:12:53.633439Z",
     "iopub.status.idle": "2024-04-21T01:48:29.331976Z",
     "shell.execute_reply": "2024-04-21T01:48:29.331005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** inicio do teste com ruido em k:4 e lambda_hard:0.0 ****\n",
      "Num GPU's: 2\n",
      "Allocated GPU's for model: [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emorais/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/emorais/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_L_32_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_L_32_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/emorais/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_M_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/emorais/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Base_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Base_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/emorais/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/emorais/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/emorais/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"/home/emorais/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\"\n",
      "Successfully loaded imagenet pretrained weights from \"/home/emorais/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emorais/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: (38400, 3)\n",
      "Gallery Size: (24000, 3)\n",
      "Query Size: (9600, 3)\n",
      "Validating visiontransformer on Jadson ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (384000x128 and 1024x1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 98\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**** inicio do teste com ruido em k:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m e lambda_hard:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambda_hard\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ****\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteste-04-ruido-norm-90epocas-MNETv3_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambda_hard\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpu_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbase_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.5e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.04\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mk1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlambda_hard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_hard\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnumber_of_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmomentum_on_feature_extraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJadson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdir_to_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdir_to_save_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43muse_ruido\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metodo \u001b[38;5;129;01min\u001b[39;00m models_name \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    101\u001b[0m     metricas_t, metricas_v, rotulos_t, rotulos_v \u001b[38;5;241m=\u001b[39m metricas(k\u001b[38;5;241m=\u001b[39mk, lambda_hard\u001b[38;5;241m=\u001b[39mlambda_hard, modelo\u001b[38;5;241m=\u001b[39mmetodo)\n",
      "File \u001b[0;32m~/repos/LESSF_ReID-working/main.py:77\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(gpu_ids, base_lr, P, K, tau, beta, k1, sampling, lambda_hard, number_of_iterations, momentum_on_feature_extraction, target, dir_to_save, dir_to_save_metrics, version, eval_freq, use_ruido)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, TOTAL_MODELOS):\n\u001b[1;32m     76\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidating \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (models_name[model], target))\n\u001b[0;32m---> 77\u001b[0m \tcmc, mAP, mat \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries_images_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgallery_images_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_online\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu_indexes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \tdistmat\u001b[38;5;241m.\u001b[39mappend(mat)\n\u001b[1;32m     80\u001b[0m distmat_ensembled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(distmat,\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/repos/LESSF_ReID-working/DCNNs.py:546\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(queries, gallery, model, rerank, gpu_index)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(queries, gallery, model, rerank\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, gpu_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    545\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 546\u001b[0m     queries_fvs \u001b[38;5;241m=\u001b[39m \u001b[43mextractFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m     gallery_fvs \u001b[38;5;241m=\u001b[39m extractFeatures(gallery, model, \u001b[38;5;241m500\u001b[39m, gpu_index)\n\u001b[1;32m    549\u001b[0m     queries_fvs \u001b[38;5;241m=\u001b[39m queries_fvs\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnorm(queries_fvs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/repos/LESSF_ReID-working/featureExtraction.py:154\u001b[0m, in \u001b[0;36mextractFeatures\u001b[0;34m(subset, model, batch_size, gpu_index, eval_mode)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    153\u001b[0m     batch_gpu \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mcuda(gpu_index)\n\u001b[0;32m--> 154\u001b[0m     fv \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_gpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     fvs \u001b[38;5;241m=\u001b[39m fv\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subset_fvs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:169\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/LESSF_ReID-working/DCNNs.py:478\u001b[0m, in \u001b[0;36mVisionTransformerReID.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 478\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    481\u001b[0m     x_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgap(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (384000x128 and 1024x1000)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from main import *\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from datasetUtils import load_from_Jadson\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# parser = argparse.ArgumentParser(description='Define the UDA parameters')\n",
    "#\n",
    "# parser.add_argument('--gpu_ids', type=str, default=\"7\", help='GPU IDs')\n",
    "# parser.add_argument('--lr', type=float, default=3.5e-4, help='Learning Rate')\n",
    "# parser.add_argument('--P', type=int, default=16, help='Number of Persons')\n",
    "# parser.add_argument('--K', type=int, default=4, help='Number of samples per person')\n",
    "# parser.add_argument('--tau', type=float, default=0.05, help='tau value used on softmax triplet loss')\n",
    "# parser.add_argument('--beta', type=float, default=0.999, help='beta used on self-Ensembling')\n",
    "# parser.add_argument('--k1', type=int, default=30, help='k on k-Reciprocal Encoding')\n",
    "# parser.add_argument('--sampling', type=str, default=\"mean\", help='Mean or Random feature vectors to be prototype')\n",
    "# parser.add_argument('--lambda_hard', type=float, default=0.5, help='tuning prameter of Softmax Triplet Loss')\n",
    "# parser.add_argument('--num_iter', type=int, default=400, help='Number of iterations on an epoch')\n",
    "# parser.add_argument('--momentum_on_feature_extraction', type=int, default=0,\n",
    "# help='If it is the momentum used on feature extraction')\n",
    "# parser.add_argument('--target', type=str, help='Name of target dataset')\n",
    "# parser.add_argument('--path_to_save_models', type=str, help='Path to save models')\n",
    "# parser.add_argument('--path_to_save_metrics', type=str, help='Path to save metrics (mAP, CMC, ...)')\n",
    "# parser.add_argument('--version', type=str, help='Path to save models')\n",
    "# parser.add_argument('--eval_freq', type=int, help='Evaluation Frequency along training')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# gpu_ids = args.gpu_ids\n",
    "# base_lr = args.lr\n",
    "# P = args.P\n",
    "# K = args.K\n",
    "\n",
    "# tau = args.tau\n",
    "# beta = args.beta\n",
    "# k1 = args.k1\n",
    "# sampling  = args.sampling\n",
    "#\n",
    "# lambda_hard = args.lambda_hard\n",
    "# number_of_iterations = args.num_iter\n",
    "# momentum_on_feature_extraction = bool(args.momentum_on_feature_extraction)\n",
    "# target = args.target\n",
    "# dir_to_save = args.path_to_save_models\n",
    "# dir_to_save_metrics = args.path_to_save_metrics\n",
    "# version = args.version\n",
    "# eval_freq = args.eval_freq\n",
    "# main.py --gpu_ids=0,1,2,3 --lr=3.5e-4 --P=16 --K=12 --tau=0.04 --beta=0.999 --k1=30 --sampling=mean --lambda_hard=0.5 --num_iter=7 --momentum_on_feature_extraction=0 --target=Duke --path_to_save_models=models --path_to_save_metrics=metrics --version=version_name --eval_freq=5\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Função para exibir a imagem usando HTML\n",
    "def exibir_imagem(imagem_path):\n",
    "    return f'<img src=\"{imagem_path}\" width=\"400\">'\n",
    "\n",
    "\n",
    "from metricas import *\n",
    "\n",
    "html_content= \"\"\n",
    "df = pd.DataFrame({\n",
    "    'k':[], \n",
    "    'lambda_hard':[],\n",
    "    'modelo':[],\n",
    "    'matriz_confusao':[], \n",
    "    'Acuracia':[], \n",
    "    'Precisao':[],\n",
    "    'Recall':[],\n",
    "    'F1-score':[],\n",
    "    'Grafico':[],\n",
    "    'Tipo':[]\n",
    "    })\n",
    "\n",
    "gpus = \"0,1\" \n",
    "for k in [4]:\n",
    "    for lambda_hard in [ 0.0 ]:\n",
    "                \n",
    "        print(f\"**** inicio do teste com ruido em k:{k} e lambda_hard:{lambda_hard} ****\")\n",
    "        \n",
    "        version = f\"teste-04-ruido-norm-90epocas-MNETv3_{k}_{lambda_hard}\"\n",
    "        \n",
    "        main(gpu_ids=gpus,base_lr=3.5e-4,P=16,K=k,tau=0.04,beta=0.999,k1=30,sampling=\"random\",lambda_hard=lambda_hard,number_of_iterations=7,momentum_on_feature_extraction=0,target=\"Jadson\",dir_to_save=\"models\",dir_to_save_metrics=\"metrics\",version=version,eval_freq=5,use_ruido=True)\n",
    "        \n",
    "        for metodo in models_name + [\"mean\"]:\n",
    "            metricas_t, metricas_v, rotulos_t, rotulos_v = metricas(k=k, lambda_hard=lambda_hard, modelo=metodo)\n",
    "            linha = {\n",
    "                'k':            [k], \n",
    "                'lambda_hard':  [lambda_hard],\n",
    "                'modelo':       [metodo],\n",
    "                'Tipo':         'Test'\n",
    "            }\n",
    "            for count in range( 0, metricas_t.shape[0] ):\n",
    "                for m in range( 0, metricas_t.shape[1] ):\n",
    "                    linha[rotulos_t[m]] = metricas_t[count][m]\n",
    "                    \n",
    "                \n",
    "                linha['matriz_confusao'] = f'resultados/MC_{k}_{lambda_hard}_{count}_{metodo}_test.png'\n",
    "                linha['Grafico'] = f'resultados/grafico_{k}_{lambda_hard}_{count}_{metodo}_test.png'\n",
    "                df = pd.concat( [df, pd.DataFrame(linha)], axis=0)\n",
    "             \n",
    "            linha = {\n",
    "               'k':             [k], \n",
    "               'lambda_hard':   [lambda_hard],\n",
    "               'modelo':        [metodo],\n",
    "               'Tipo':          'Valid'\n",
    "             }\n",
    "            for count in range( 0, metricas_v.shape[0] ):\n",
    "                for m in range( 0, metricas_v.shape[1] ):\n",
    "                    linha[rotulos_v[m]] = metricas_v[count][m] \n",
    "               \n",
    "                linha['Grafico'] = f'resultados/grafico_{k}_{lambda_hard}_{count}_{metodo}_valid.png'\n",
    "                linha['matriz_confusao'] = f'resultados/MC_{k}_{lambda_hard}_{count}_{metodo}_valid.png' \n",
    "                df = pd.concat( [df, pd.DataFrame(linha)], axis=0)\n",
    "        \n",
    "        # Aplicar a função à coluna 'imagem' e criar uma nova coluna 'imagem_exibicao'\n",
    "        df['MC'] = df['matriz_confusao'].apply(exibir_imagem)\n",
    "        df['GR'] = df['Grafico'].apply(exibir_imagem)\n",
    "        \n",
    "        html_content = df[['k', \n",
    "                           'lambda_hard', \n",
    "                           'Tipo', \n",
    "                           'modelo'] + \n",
    "                           rotulos_v[:8] + \n",
    "                           ['MC', \n",
    "                           'GR']].to_html(escape=False, index=False)\n",
    "        # salvando df em arquivo html\n",
    "        # Use BeautifulSoup para formatar o HTML\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        formatted_html = soup.prettify()\n",
    "        \n",
    "        # Salve o HTML em um arquivo\n",
    "        head = \"<!DOCTYPE html>\\n<html lang='pt-br'>\\n<head>\\n  <meta charset='UTF-8'>\\n  <meta name='viewport' content='width=device-width, initial-scale=1.0'>\\n  <style>\\n    table {\\n      width: 100%;\\n      border-collapse: collapse;\\n    }\\n    th, td {\\n      border: 1px solid #ddd;\\n      padding: 8px;\\n      text-align: left;\\n    }\\n    th {\\n      background-color: #f2f2f2;\\n    }\\n    thead th {\\n      position: sticky;\\n      top: 0;\\n      z-index: 1;\\n      background-color: #f2f2f2;    }\\n  </style>\\n    <title>Relatório Parcial</title>\\n</head>\\n<body>\"\n",
    "        with open('relatorio-APCER-BPCER-ACER-silhouette-ward-90epocas-ruido-norm-MNETv3-convnet.html', 'w', encoding='utf-8') as file:\n",
    "            file.write(head)\n",
    "            file.write(formatted_html)\n",
    "            file.write('</body></html>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T01:48:29.340909Z",
     "iopub.status.busy": "2024-04-21T01:48:29.340213Z",
     "iopub.status.idle": "2024-04-21T01:48:29.357721Z",
     "shell.execute_reply": "2024-04-21T01:48:29.357003Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exibir o DataFrame com as imagens\n",
    "display(HTML(html_content))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "(['x', 'getattr', 'getitem', 'getitem_1', 'getitem_2', 'getitem_3', 'eq', '_assert', 'eq_1', '_assert_1', 'floordiv', 'floordiv_1', 'conv_proj', 'mul', 'reshape', 'permute', 'getattr_1', 'getitem_4', 'class_token', 'expand', 'cat', 'encoder.dim', 'encoder.eq', 'encoder.getattr', 'encoder._assert', 'encoder.encoder_pos_embedding', 'encoder.add', 'encoder.dropout', 'encoder.layers.encoder_layer_0.dim', 'encoder.layers.encoder_layer_0.eq', 'encoder.layers.encoder_layer_0.getattr', 'encoder.layers.encoder_layer_0._assert', 'encoder.layers.encoder_layer_0.ln', 'encoder.layers.encoder_layer_0.self_attention', 'encoder.layers.encoder_layer_0.getitem', 'encoder.layers.encoder_layer_0.getitem_1', 'encoder.layers.encoder_layer_0.dropout', 'encoder.layers.encoder_layer_0.add', 'encoder.layers.encoder_layer_0.ln_1', 'encoder.layers.encoder_layer_0.mlp', 'encoder.layers.encoder_layer_0.add_1', 'encoder.layers.encoder_layer_1.dim', 'encoder.layers.encoder_layer_1.eq', 'encoder.layers.encoder_layer_1.getattr', 'encoder.layers.encoder_layer_1._assert', 'encoder.layers.encoder_layer_1.ln', 'encoder.layers.encoder_layer_1.self_attention', 'encoder.layers.encoder_layer_1.getitem', 'encoder.layers.encoder_layer_1.getitem_1', 'encoder.layers.encoder_layer_1.dropout', 'encoder.layers.encoder_layer_1.add', 'encoder.layers.encoder_layer_1.ln_1', 'encoder.layers.encoder_layer_1.mlp', 'encoder.layers.encoder_layer_1.add_1', 'encoder.layers.encoder_layer_2.dim', 'encoder.layers.encoder_layer_2.eq', 'encoder.layers.encoder_layer_2.getattr', 'encoder.layers.encoder_layer_2._assert', 'encoder.layers.encoder_layer_2.ln', 'encoder.layers.encoder_layer_2.self_attention', 'encoder.layers.encoder_layer_2.getitem', 'encoder.layers.encoder_layer_2.getitem_1', 'encoder.layers.encoder_layer_2.dropout', 'encoder.layers.encoder_layer_2.add', 'encoder.layers.encoder_layer_2.ln_1', 'encoder.layers.encoder_layer_2.mlp', 'encoder.layers.encoder_layer_2.add_1', 'encoder.layers.encoder_layer_3.dim', 'encoder.layers.encoder_layer_3.eq', 'encoder.layers.encoder_layer_3.getattr', 'encoder.layers.encoder_layer_3._assert', 'encoder.layers.encoder_layer_3.ln', 'encoder.layers.encoder_layer_3.self_attention', 'encoder.layers.encoder_layer_3.getitem', 'encoder.layers.encoder_layer_3.getitem_1', 'encoder.layers.encoder_layer_3.dropout', 'encoder.layers.encoder_layer_3.add', 'encoder.layers.encoder_layer_3.ln_1', 'encoder.layers.encoder_layer_3.mlp', 'encoder.layers.encoder_layer_3.add_1', 'encoder.layers.encoder_layer_4.dim', 'encoder.layers.encoder_layer_4.eq', 'encoder.layers.encoder_layer_4.getattr', 'encoder.layers.encoder_layer_4._assert', 'encoder.layers.encoder_layer_4.ln', 'encoder.layers.encoder_layer_4.self_attention', 'encoder.layers.encoder_layer_4.getitem', 'encoder.layers.encoder_layer_4.getitem_1', 'encoder.layers.encoder_layer_4.dropout', 'encoder.layers.encoder_layer_4.add', 'encoder.layers.encoder_layer_4.ln_1', 'encoder.layers.encoder_layer_4.mlp', 'encoder.layers.encoder_layer_4.add_1', 'encoder.layers.encoder_layer_5.dim', 'encoder.layers.encoder_layer_5.eq', 'encoder.layers.encoder_layer_5.getattr', 'encoder.layers.encoder_layer_5._assert', 'encoder.layers.encoder_layer_5.ln', 'encoder.layers.encoder_layer_5.self_attention', 'encoder.layers.encoder_layer_5.getitem', 'encoder.layers.encoder_layer_5.getitem_1', 'encoder.layers.encoder_layer_5.dropout', 'encoder.layers.encoder_layer_5.add', 'encoder.layers.encoder_layer_5.ln_1', 'encoder.layers.encoder_layer_5.mlp', 'encoder.layers.encoder_layer_5.add_1', 'encoder.layers.encoder_layer_6.dim', 'encoder.layers.encoder_layer_6.eq', 'encoder.layers.encoder_layer_6.getattr', 'encoder.layers.encoder_layer_6._assert', 'encoder.layers.encoder_layer_6.ln', 'encoder.layers.encoder_layer_6.self_attention', 'encoder.layers.encoder_layer_6.getitem', 'encoder.layers.encoder_layer_6.getitem_1', 'encoder.layers.encoder_layer_6.dropout', 'encoder.layers.encoder_layer_6.add', 'encoder.layers.encoder_layer_6.ln_1', 'encoder.layers.encoder_layer_6.mlp', 'encoder.layers.encoder_layer_6.add_1', 'encoder.layers.encoder_layer_7.dim', 'encoder.layers.encoder_layer_7.eq', 'encoder.layers.encoder_layer_7.getattr', 'encoder.layers.encoder_layer_7._assert', 'encoder.layers.encoder_layer_7.ln', 'encoder.layers.encoder_layer_7.self_attention', 'encoder.layers.encoder_layer_7.getitem', 'encoder.layers.encoder_layer_7.getitem_1', 'encoder.layers.encoder_layer_7.dropout', 'encoder.layers.encoder_layer_7.add', 'encoder.layers.encoder_layer_7.ln_1', 'encoder.layers.encoder_layer_7.mlp', 'encoder.layers.encoder_layer_7.add_1', 'encoder.layers.encoder_layer_8.dim', 'encoder.layers.encoder_layer_8.eq', 'encoder.layers.encoder_layer_8.getattr', 'encoder.layers.encoder_layer_8._assert', 'encoder.layers.encoder_layer_8.ln', 'encoder.layers.encoder_layer_8.self_attention', 'encoder.layers.encoder_layer_8.getitem', 'encoder.layers.encoder_layer_8.getitem_1', 'encoder.layers.encoder_layer_8.dropout', 'encoder.layers.encoder_layer_8.add', 'encoder.layers.encoder_layer_8.ln_1', 'encoder.layers.encoder_layer_8.mlp', 'encoder.layers.encoder_layer_8.add_1', 'encoder.layers.encoder_layer_9.dim', 'encoder.layers.encoder_layer_9.eq', 'encoder.layers.encoder_layer_9.getattr', 'encoder.layers.encoder_layer_9._assert', 'encoder.layers.encoder_layer_9.ln', 'encoder.layers.encoder_layer_9.self_attention', 'encoder.layers.encoder_layer_9.getitem', 'encoder.layers.encoder_layer_9.getitem_1', 'encoder.layers.encoder_layer_9.dropout', 'encoder.layers.encoder_layer_9.add', 'encoder.layers.encoder_layer_9.ln_1', 'encoder.layers.encoder_layer_9.mlp', 'encoder.layers.encoder_layer_9.add_1', 'encoder.layers.encoder_layer_10.dim', 'encoder.layers.encoder_layer_10.eq', 'encoder.layers.encoder_layer_10.getattr', 'encoder.layers.encoder_layer_10._assert', 'encoder.layers.encoder_layer_10.ln', 'encoder.layers.encoder_layer_10.self_attention', 'encoder.layers.encoder_layer_10.getitem', 'encoder.layers.encoder_layer_10.getitem_1', 'encoder.layers.encoder_layer_10.dropout', 'encoder.layers.encoder_layer_10.add', 'encoder.layers.encoder_layer_10.ln_1', 'encoder.layers.encoder_layer_10.mlp', 'encoder.layers.encoder_layer_10.add_1', 'encoder.layers.encoder_layer_11.dim', 'encoder.layers.encoder_layer_11.eq', 'encoder.layers.encoder_layer_11.getattr', 'encoder.layers.encoder_layer_11._assert', 'encoder.layers.encoder_layer_11.ln', 'encoder.layers.encoder_layer_11.self_attention', 'encoder.layers.encoder_layer_11.getitem', 'encoder.layers.encoder_layer_11.getitem_1', 'encoder.layers.encoder_layer_11.dropout', 'encoder.layers.encoder_layer_11.add', 'encoder.layers.encoder_layer_11.ln_1', 'encoder.layers.encoder_layer_11.mlp', 'encoder.layers.encoder_layer_11.add_1', 'encoder.layers.encoder_layer_12.dim', 'encoder.layers.encoder_layer_12.eq', 'encoder.layers.encoder_layer_12.getattr', 'encoder.layers.encoder_layer_12._assert', 'encoder.layers.encoder_layer_12.ln', 'encoder.layers.encoder_layer_12.self_attention', 'encoder.layers.encoder_layer_12.getitem', 'encoder.layers.encoder_layer_12.getitem_1', 'encoder.layers.encoder_layer_12.dropout', 'encoder.layers.encoder_layer_12.add', 'encoder.layers.encoder_layer_12.ln_1', 'encoder.layers.encoder_layer_12.mlp', 'encoder.layers.encoder_layer_12.add_1', 'encoder.layers.encoder_layer_13.dim', 'encoder.layers.encoder_layer_13.eq', 'encoder.layers.encoder_layer_13.getattr', 'encoder.layers.encoder_layer_13._assert', 'encoder.layers.encoder_layer_13.ln', 'encoder.layers.encoder_layer_13.self_attention', 'encoder.layers.encoder_layer_13.getitem', 'encoder.layers.encoder_layer_13.getitem_1', 'encoder.layers.encoder_layer_13.dropout', 'encoder.layers.encoder_layer_13.add', 'encoder.layers.encoder_layer_13.ln_1', 'encoder.layers.encoder_layer_13.mlp', 'encoder.layers.encoder_layer_13.add_1', 'encoder.layers.encoder_layer_14.dim', 'encoder.layers.encoder_layer_14.eq', 'encoder.layers.encoder_layer_14.getattr', 'encoder.layers.encoder_layer_14._assert', 'encoder.layers.encoder_layer_14.ln', 'encoder.layers.encoder_layer_14.self_attention', 'encoder.layers.encoder_layer_14.getitem', 'encoder.layers.encoder_layer_14.getitem_1', 'encoder.layers.encoder_layer_14.dropout', 'encoder.layers.encoder_layer_14.add', 'encoder.layers.encoder_layer_14.ln_1', 'encoder.layers.encoder_layer_14.mlp', 'encoder.layers.encoder_layer_14.add_1', 'encoder.layers.encoder_layer_15.dim', 'encoder.layers.encoder_layer_15.eq', 'encoder.layers.encoder_layer_15.getattr', 'encoder.layers.encoder_layer_15._assert', 'encoder.layers.encoder_layer_15.ln', 'encoder.layers.encoder_layer_15.self_attention', 'encoder.layers.encoder_layer_15.getitem', 'encoder.layers.encoder_layer_15.getitem_1', 'encoder.layers.encoder_layer_15.dropout', 'encoder.layers.encoder_layer_15.add', 'encoder.layers.encoder_layer_15.ln_1', 'encoder.layers.encoder_layer_15.mlp', 'encoder.layers.encoder_layer_15.add_1', 'encoder.layers.encoder_layer_16.dim', 'encoder.layers.encoder_layer_16.eq', 'encoder.layers.encoder_layer_16.getattr', 'encoder.layers.encoder_layer_16._assert', 'encoder.layers.encoder_layer_16.ln', 'encoder.layers.encoder_layer_16.self_attention', 'encoder.layers.encoder_layer_16.getitem', 'encoder.layers.encoder_layer_16.getitem_1', 'encoder.layers.encoder_layer_16.dropout', 'encoder.layers.encoder_layer_16.add', 'encoder.layers.encoder_layer_16.ln_1', 'encoder.layers.encoder_layer_16.mlp', 'encoder.layers.encoder_layer_16.add_1', 'encoder.layers.encoder_layer_17.dim', 'encoder.layers.encoder_layer_17.eq', 'encoder.layers.encoder_layer_17.getattr', 'encoder.layers.encoder_layer_17._assert', 'encoder.layers.encoder_layer_17.ln', 'encoder.layers.encoder_layer_17.self_attention', 'encoder.layers.encoder_layer_17.getitem', 'encoder.layers.encoder_layer_17.getitem_1', 'encoder.layers.encoder_layer_17.dropout', 'encoder.layers.encoder_layer_17.add', 'encoder.layers.encoder_layer_17.ln_1', 'encoder.layers.encoder_layer_17.mlp', 'encoder.layers.encoder_layer_17.add_1', 'encoder.layers.encoder_layer_18.dim', 'encoder.layers.encoder_layer_18.eq', 'encoder.layers.encoder_layer_18.getattr', 'encoder.layers.encoder_layer_18._assert', 'encoder.layers.encoder_layer_18.ln', 'encoder.layers.encoder_layer_18.self_attention', 'encoder.layers.encoder_layer_18.getitem', 'encoder.layers.encoder_layer_18.getitem_1', 'encoder.layers.encoder_layer_18.dropout', 'encoder.layers.encoder_layer_18.add', 'encoder.layers.encoder_layer_18.ln_1', 'encoder.layers.encoder_layer_18.mlp', 'encoder.layers.encoder_layer_18.add_1', 'encoder.layers.encoder_layer_19.dim', 'encoder.layers.encoder_layer_19.eq', 'encoder.layers.encoder_layer_19.getattr', 'encoder.layers.encoder_layer_19._assert', 'encoder.layers.encoder_layer_19.ln', 'encoder.layers.encoder_layer_19.self_attention', 'encoder.layers.encoder_layer_19.getitem', 'encoder.layers.encoder_layer_19.getitem_1', 'encoder.layers.encoder_layer_19.dropout', 'encoder.layers.encoder_layer_19.add', 'encoder.layers.encoder_layer_19.ln_1', 'encoder.layers.encoder_layer_19.mlp', 'encoder.layers.encoder_layer_19.add_1', 'encoder.layers.encoder_layer_20.dim', 'encoder.layers.encoder_layer_20.eq', 'encoder.layers.encoder_layer_20.getattr', 'encoder.layers.encoder_layer_20._assert', 'encoder.layers.encoder_layer_20.ln', 'encoder.layers.encoder_layer_20.self_attention', 'encoder.layers.encoder_layer_20.getitem', 'encoder.layers.encoder_layer_20.getitem_1', 'encoder.layers.encoder_layer_20.dropout', 'encoder.layers.encoder_layer_20.add', 'encoder.layers.encoder_layer_20.ln_1', 'encoder.layers.encoder_layer_20.mlp', 'encoder.layers.encoder_layer_20.add_1', 'encoder.layers.encoder_layer_21.dim', 'encoder.layers.encoder_layer_21.eq', 'encoder.layers.encoder_layer_21.getattr', 'encoder.layers.encoder_layer_21._assert', 'encoder.layers.encoder_layer_21.ln', 'encoder.layers.encoder_layer_21.self_attention', 'encoder.layers.encoder_layer_21.getitem', 'encoder.layers.encoder_layer_21.getitem_1', 'encoder.layers.encoder_layer_21.dropout', 'encoder.layers.encoder_layer_21.add', 'encoder.layers.encoder_layer_21.ln_1', 'encoder.layers.encoder_layer_21.mlp', 'encoder.layers.encoder_layer_21.add_1', 'encoder.layers.encoder_layer_22.dim', 'encoder.layers.encoder_layer_22.eq', 'encoder.layers.encoder_layer_22.getattr', 'encoder.layers.encoder_layer_22._assert', 'encoder.layers.encoder_layer_22.ln', 'encoder.layers.encoder_layer_22.self_attention', 'encoder.layers.encoder_layer_22.getitem', 'encoder.layers.encoder_layer_22.getitem_1', 'encoder.layers.encoder_layer_22.dropout', 'encoder.layers.encoder_layer_22.add', 'encoder.layers.encoder_layer_22.ln_1', 'encoder.layers.encoder_layer_22.mlp', 'encoder.layers.encoder_layer_22.add_1', 'encoder.layers.encoder_layer_23.dim', 'encoder.layers.encoder_layer_23.eq', 'encoder.layers.encoder_layer_23.getattr', 'encoder.layers.encoder_layer_23._assert', 'encoder.layers.encoder_layer_23.ln', 'encoder.layers.encoder_layer_23.self_attention', 'encoder.layers.encoder_layer_23.getitem', 'encoder.layers.encoder_layer_23.getitem_1', 'encoder.layers.encoder_layer_23.dropout', 'encoder.layers.encoder_layer_23.add', 'encoder.layers.encoder_layer_23.ln_1', 'encoder.layers.encoder_layer_23.mlp', 'encoder.layers.encoder_layer_23.add_1', 'encoder.ln', 'getitem_5', 'heads.head'], ['x', 'getattr', 'getitem', 'getitem_1', 'getitem_2', 'getitem_3', 'eq', '_assert', 'eq_1', '_assert_1', 'floordiv', 'floordiv_1', 'conv_proj', 'mul', 'reshape', 'permute', 'getattr_1', 'getitem_4', 'class_token', 'expand', 'cat', 'encoder.dim', 'encoder.eq', 'encoder.getattr', 'encoder._assert', 'encoder.encoder_pos_embedding', 'encoder.add', 'encoder.dropout', 'encoder.layers.encoder_layer_0.dim', 'encoder.layers.encoder_layer_0.eq', 'encoder.layers.encoder_layer_0.getattr', 'encoder.layers.encoder_layer_0._assert', 'encoder.layers.encoder_layer_0.ln', 'encoder.layers.encoder_layer_0.self_attention', 'encoder.layers.encoder_layer_0.getitem', 'encoder.layers.encoder_layer_0.getitem_1', 'encoder.layers.encoder_layer_0.dropout', 'encoder.layers.encoder_layer_0.add', 'encoder.layers.encoder_layer_0.ln_1', 'encoder.layers.encoder_layer_0.mlp', 'encoder.layers.encoder_layer_0.add_1', 'encoder.layers.encoder_layer_1.dim', 'encoder.layers.encoder_layer_1.eq', 'encoder.layers.encoder_layer_1.getattr', 'encoder.layers.encoder_layer_1._assert', 'encoder.layers.encoder_layer_1.ln', 'encoder.layers.encoder_layer_1.self_attention', 'encoder.layers.encoder_layer_1.getitem', 'encoder.layers.encoder_layer_1.getitem_1', 'encoder.layers.encoder_layer_1.dropout', 'encoder.layers.encoder_layer_1.add', 'encoder.layers.encoder_layer_1.ln_1', 'encoder.layers.encoder_layer_1.mlp', 'encoder.layers.encoder_layer_1.add_1', 'encoder.layers.encoder_layer_2.dim', 'encoder.layers.encoder_layer_2.eq', 'encoder.layers.encoder_layer_2.getattr', 'encoder.layers.encoder_layer_2._assert', 'encoder.layers.encoder_layer_2.ln', 'encoder.layers.encoder_layer_2.self_attention', 'encoder.layers.encoder_layer_2.getitem', 'encoder.layers.encoder_layer_2.getitem_1', 'encoder.layers.encoder_layer_2.dropout', 'encoder.layers.encoder_layer_2.add', 'encoder.layers.encoder_layer_2.ln_1', 'encoder.layers.encoder_layer_2.mlp', 'encoder.layers.encoder_layer_2.add_1', 'encoder.layers.encoder_layer_3.dim', 'encoder.layers.encoder_layer_3.eq', 'encoder.layers.encoder_layer_3.getattr', 'encoder.layers.encoder_layer_3._assert', 'encoder.layers.encoder_layer_3.ln', 'encoder.layers.encoder_layer_3.self_attention', 'encoder.layers.encoder_layer_3.getitem', 'encoder.layers.encoder_layer_3.getitem_1', 'encoder.layers.encoder_layer_3.dropout', 'encoder.layers.encoder_layer_3.add', 'encoder.layers.encoder_layer_3.ln_1', 'encoder.layers.encoder_layer_3.mlp', 'encoder.layers.encoder_layer_3.add_1', 'encoder.layers.encoder_layer_4.dim', 'encoder.layers.encoder_layer_4.eq', 'encoder.layers.encoder_layer_4.getattr', 'encoder.layers.encoder_layer_4._assert', 'encoder.layers.encoder_layer_4.ln', 'encoder.layers.encoder_layer_4.self_attention', 'encoder.layers.encoder_layer_4.getitem', 'encoder.layers.encoder_layer_4.getitem_1', 'encoder.layers.encoder_layer_4.dropout', 'encoder.layers.encoder_layer_4.add', 'encoder.layers.encoder_layer_4.ln_1', 'encoder.layers.encoder_layer_4.mlp', 'encoder.layers.encoder_layer_4.add_1', 'encoder.layers.encoder_layer_5.dim', 'encoder.layers.encoder_layer_5.eq', 'encoder.layers.encoder_layer_5.getattr', 'encoder.layers.encoder_layer_5._assert', 'encoder.layers.encoder_layer_5.ln', 'encoder.layers.encoder_layer_5.self_attention', 'encoder.layers.encoder_layer_5.getitem', 'encoder.layers.encoder_layer_5.getitem_1', 'encoder.layers.encoder_layer_5.dropout', 'encoder.layers.encoder_layer_5.add', 'encoder.layers.encoder_layer_5.ln_1', 'encoder.layers.encoder_layer_5.mlp', 'encoder.layers.encoder_layer_5.add_1', 'encoder.layers.encoder_layer_6.dim', 'encoder.layers.encoder_layer_6.eq', 'encoder.layers.encoder_layer_6.getattr', 'encoder.layers.encoder_layer_6._assert', 'encoder.layers.encoder_layer_6.ln', 'encoder.layers.encoder_layer_6.self_attention', 'encoder.layers.encoder_layer_6.getitem', 'encoder.layers.encoder_layer_6.getitem_1', 'encoder.layers.encoder_layer_6.dropout', 'encoder.layers.encoder_layer_6.add', 'encoder.layers.encoder_layer_6.ln_1', 'encoder.layers.encoder_layer_6.mlp', 'encoder.layers.encoder_layer_6.add_1', 'encoder.layers.encoder_layer_7.dim', 'encoder.layers.encoder_layer_7.eq', 'encoder.layers.encoder_layer_7.getattr', 'encoder.layers.encoder_layer_7._assert', 'encoder.layers.encoder_layer_7.ln', 'encoder.layers.encoder_layer_7.self_attention', 'encoder.layers.encoder_layer_7.getitem', 'encoder.layers.encoder_layer_7.getitem_1', 'encoder.layers.encoder_layer_7.dropout', 'encoder.layers.encoder_layer_7.add', 'encoder.layers.encoder_layer_7.ln_1', 'encoder.layers.encoder_layer_7.mlp', 'encoder.layers.encoder_layer_7.add_1', 'encoder.layers.encoder_layer_8.dim', 'encoder.layers.encoder_layer_8.eq', 'encoder.layers.encoder_layer_8.getattr', 'encoder.layers.encoder_layer_8._assert', 'encoder.layers.encoder_layer_8.ln', 'encoder.layers.encoder_layer_8.self_attention', 'encoder.layers.encoder_layer_8.getitem', 'encoder.layers.encoder_layer_8.getitem_1', 'encoder.layers.encoder_layer_8.dropout', 'encoder.layers.encoder_layer_8.add', 'encoder.layers.encoder_layer_8.ln_1', 'encoder.layers.encoder_layer_8.mlp', 'encoder.layers.encoder_layer_8.add_1', 'encoder.layers.encoder_layer_9.dim', 'encoder.layers.encoder_layer_9.eq', 'encoder.layers.encoder_layer_9.getattr', 'encoder.layers.encoder_layer_9._assert', 'encoder.layers.encoder_layer_9.ln', 'encoder.layers.encoder_layer_9.self_attention', 'encoder.layers.encoder_layer_9.getitem', 'encoder.layers.encoder_layer_9.getitem_1', 'encoder.layers.encoder_layer_9.dropout', 'encoder.layers.encoder_layer_9.add', 'encoder.layers.encoder_layer_9.ln_1', 'encoder.layers.encoder_layer_9.mlp', 'encoder.layers.encoder_layer_9.add_1', 'encoder.layers.encoder_layer_10.dim', 'encoder.layers.encoder_layer_10.eq', 'encoder.layers.encoder_layer_10.getattr', 'encoder.layers.encoder_layer_10._assert', 'encoder.layers.encoder_layer_10.ln', 'encoder.layers.encoder_layer_10.self_attention', 'encoder.layers.encoder_layer_10.getitem', 'encoder.layers.encoder_layer_10.getitem_1', 'encoder.layers.encoder_layer_10.dropout', 'encoder.layers.encoder_layer_10.add', 'encoder.layers.encoder_layer_10.ln_1', 'encoder.layers.encoder_layer_10.mlp', 'encoder.layers.encoder_layer_10.add_1', 'encoder.layers.encoder_layer_11.dim', 'encoder.layers.encoder_layer_11.eq', 'encoder.layers.encoder_layer_11.getattr', 'encoder.layers.encoder_layer_11._assert', 'encoder.layers.encoder_layer_11.ln', 'encoder.layers.encoder_layer_11.self_attention', 'encoder.layers.encoder_layer_11.getitem', 'encoder.layers.encoder_layer_11.getitem_1', 'encoder.layers.encoder_layer_11.dropout', 'encoder.layers.encoder_layer_11.add', 'encoder.layers.encoder_layer_11.ln_1', 'encoder.layers.encoder_layer_11.mlp', 'encoder.layers.encoder_layer_11.add_1', 'encoder.layers.encoder_layer_12.dim', 'encoder.layers.encoder_layer_12.eq', 'encoder.layers.encoder_layer_12.getattr', 'encoder.layers.encoder_layer_12._assert', 'encoder.layers.encoder_layer_12.ln', 'encoder.layers.encoder_layer_12.self_attention', 'encoder.layers.encoder_layer_12.getitem', 'encoder.layers.encoder_layer_12.getitem_1', 'encoder.layers.encoder_layer_12.dropout', 'encoder.layers.encoder_layer_12.add', 'encoder.layers.encoder_layer_12.ln_1', 'encoder.layers.encoder_layer_12.mlp', 'encoder.layers.encoder_layer_12.add_1', 'encoder.layers.encoder_layer_13.dim', 'encoder.layers.encoder_layer_13.eq', 'encoder.layers.encoder_layer_13.getattr', 'encoder.layers.encoder_layer_13._assert', 'encoder.layers.encoder_layer_13.ln', 'encoder.layers.encoder_layer_13.self_attention', 'encoder.layers.encoder_layer_13.getitem', 'encoder.layers.encoder_layer_13.getitem_1', 'encoder.layers.encoder_layer_13.dropout', 'encoder.layers.encoder_layer_13.add', 'encoder.layers.encoder_layer_13.ln_1', 'encoder.layers.encoder_layer_13.mlp', 'encoder.layers.encoder_layer_13.add_1', 'encoder.layers.encoder_layer_14.dim', 'encoder.layers.encoder_layer_14.eq', 'encoder.layers.encoder_layer_14.getattr', 'encoder.layers.encoder_layer_14._assert', 'encoder.layers.encoder_layer_14.ln', 'encoder.layers.encoder_layer_14.self_attention', 'encoder.layers.encoder_layer_14.getitem', 'encoder.layers.encoder_layer_14.getitem_1', 'encoder.layers.encoder_layer_14.dropout', 'encoder.layers.encoder_layer_14.add', 'encoder.layers.encoder_layer_14.ln_1', 'encoder.layers.encoder_layer_14.mlp', 'encoder.layers.encoder_layer_14.add_1', 'encoder.layers.encoder_layer_15.dim', 'encoder.layers.encoder_layer_15.eq', 'encoder.layers.encoder_layer_15.getattr', 'encoder.layers.encoder_layer_15._assert', 'encoder.layers.encoder_layer_15.ln', 'encoder.layers.encoder_layer_15.self_attention', 'encoder.layers.encoder_layer_15.getitem', 'encoder.layers.encoder_layer_15.getitem_1', 'encoder.layers.encoder_layer_15.dropout', 'encoder.layers.encoder_layer_15.add', 'encoder.layers.encoder_layer_15.ln_1', 'encoder.layers.encoder_layer_15.mlp', 'encoder.layers.encoder_layer_15.add_1', 'encoder.layers.encoder_layer_16.dim', 'encoder.layers.encoder_layer_16.eq', 'encoder.layers.encoder_layer_16.getattr', 'encoder.layers.encoder_layer_16._assert', 'encoder.layers.encoder_layer_16.ln', 'encoder.layers.encoder_layer_16.self_attention', 'encoder.layers.encoder_layer_16.getitem', 'encoder.layers.encoder_layer_16.getitem_1', 'encoder.layers.encoder_layer_16.dropout', 'encoder.layers.encoder_layer_16.add', 'encoder.layers.encoder_layer_16.ln_1', 'encoder.layers.encoder_layer_16.mlp', 'encoder.layers.encoder_layer_16.add_1', 'encoder.layers.encoder_layer_17.dim', 'encoder.layers.encoder_layer_17.eq', 'encoder.layers.encoder_layer_17.getattr', 'encoder.layers.encoder_layer_17._assert', 'encoder.layers.encoder_layer_17.ln', 'encoder.layers.encoder_layer_17.self_attention', 'encoder.layers.encoder_layer_17.getitem', 'encoder.layers.encoder_layer_17.getitem_1', 'encoder.layers.encoder_layer_17.dropout', 'encoder.layers.encoder_layer_17.add', 'encoder.layers.encoder_layer_17.ln_1', 'encoder.layers.encoder_layer_17.mlp', 'encoder.layers.encoder_layer_17.add_1', 'encoder.layers.encoder_layer_18.dim', 'encoder.layers.encoder_layer_18.eq', 'encoder.layers.encoder_layer_18.getattr', 'encoder.layers.encoder_layer_18._assert', 'encoder.layers.encoder_layer_18.ln', 'encoder.layers.encoder_layer_18.self_attention', 'encoder.layers.encoder_layer_18.getitem', 'encoder.layers.encoder_layer_18.getitem_1', 'encoder.layers.encoder_layer_18.dropout', 'encoder.layers.encoder_layer_18.add', 'encoder.layers.encoder_layer_18.ln_1', 'encoder.layers.encoder_layer_18.mlp', 'encoder.layers.encoder_layer_18.add_1', 'encoder.layers.encoder_layer_19.dim', 'encoder.layers.encoder_layer_19.eq', 'encoder.layers.encoder_layer_19.getattr', 'encoder.layers.encoder_layer_19._assert', 'encoder.layers.encoder_layer_19.ln', 'encoder.layers.encoder_layer_19.self_attention', 'encoder.layers.encoder_layer_19.getitem', 'encoder.layers.encoder_layer_19.getitem_1', 'encoder.layers.encoder_layer_19.dropout', 'encoder.layers.encoder_layer_19.add', 'encoder.layers.encoder_layer_19.ln_1', 'encoder.layers.encoder_layer_19.mlp', 'encoder.layers.encoder_layer_19.add_1', 'encoder.layers.encoder_layer_20.dim', 'encoder.layers.encoder_layer_20.eq', 'encoder.layers.encoder_layer_20.getattr', 'encoder.layers.encoder_layer_20._assert', 'encoder.layers.encoder_layer_20.ln', 'encoder.layers.encoder_layer_20.self_attention', 'encoder.layers.encoder_layer_20.getitem', 'encoder.layers.encoder_layer_20.getitem_1', 'encoder.layers.encoder_layer_20.dropout', 'encoder.layers.encoder_layer_20.add', 'encoder.layers.encoder_layer_20.ln_1', 'encoder.layers.encoder_layer_20.mlp', 'encoder.layers.encoder_layer_20.add_1', 'encoder.layers.encoder_layer_21.dim', 'encoder.layers.encoder_layer_21.eq', 'encoder.layers.encoder_layer_21.getattr', 'encoder.layers.encoder_layer_21._assert', 'encoder.layers.encoder_layer_21.ln', 'encoder.layers.encoder_layer_21.self_attention', 'encoder.layers.encoder_layer_21.getitem', 'encoder.layers.encoder_layer_21.getitem_1', 'encoder.layers.encoder_layer_21.dropout', 'encoder.layers.encoder_layer_21.add', 'encoder.layers.encoder_layer_21.ln_1', 'encoder.layers.encoder_layer_21.mlp', 'encoder.layers.encoder_layer_21.add_1', 'encoder.layers.encoder_layer_22.dim', 'encoder.layers.encoder_layer_22.eq', 'encoder.layers.encoder_layer_22.getattr', 'encoder.layers.encoder_layer_22._assert', 'encoder.layers.encoder_layer_22.ln', 'encoder.layers.encoder_layer_22.self_attention', 'encoder.layers.encoder_layer_22.getitem', 'encoder.layers.encoder_layer_22.getitem_1', 'encoder.layers.encoder_layer_22.dropout', 'encoder.layers.encoder_layer_22.add', 'encoder.layers.encoder_layer_22.ln_1', 'encoder.layers.encoder_layer_22.mlp', 'encoder.layers.encoder_layer_22.add_1', 'encoder.layers.encoder_layer_23.dim', 'encoder.layers.encoder_layer_23.eq', 'encoder.layers.encoder_layer_23.getattr', 'encoder.layers.encoder_layer_23._assert', 'encoder.layers.encoder_layer_23.ln', 'encoder.layers.encoder_layer_23.self_attention', 'encoder.layers.encoder_layer_23.getitem', 'encoder.layers.encoder_layer_23.getitem_1', 'encoder.layers.encoder_layer_23.dropout', 'encoder.layers.encoder_layer_23.add', 'encoder.layers.encoder_layer_23.ln_1', 'encoder.layers.encoder_layer_23.mlp', 'encoder.layers.encoder_layer_23.add_1', 'encoder.ln', 'getitem_5', 'heads.head'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchreid\n",
    "import torchvision\n",
    "from torchvision.models import *\n",
    "from torch.nn import Module, Dropout, BatchNorm1d, Linear, AdaptiveAvgPool2d, CrossEntropyLoss, Softmax, ReLU, AdaptiveMaxPool2d\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import torchvision.models.feature_extraction\n",
    "\n",
    "from featureExtraction import extractFeatures, extractTextFeatures\n",
    "\n",
    "import warnings\n",
    "\n",
    "model = vit_l_32(pretrained=True)\n",
    "\n",
    "print (model.encoder.ln)\n",
    "print(torchvision.models.feature_extraction.get_graph_node_names(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
